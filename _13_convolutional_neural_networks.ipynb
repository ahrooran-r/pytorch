{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA GeForce GTX 1650 with Max-Q Design'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "gpu = torch.cuda.current_device()\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# dataset import\n",
    "transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data/cifar\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data/cifar\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# name classes\n",
    "classes = [\n",
    "    \"plane\", \"car\", \"bird\", \"cat\",\n",
    "    \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 3, 32, 32])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "samples.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`torch.Size([4, 3, 32, 32])`\n",
    "\n",
    "1. batch_size = 4\n",
    "2. channel = 3 (RGB)\n",
    "3. image = 32 x 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # un-normalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(samples[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# setup model\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # 3 -> 3 channels\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # 16 * 5 * 5 -> must be fixed\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MyCNN().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# setup loss, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OpenSoftware\\miniconda\\envs\\ml_project\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/4, step: 1/12500, loss: 2.3811\n",
      "epoch: 1/4, step: 1001/12500, loss: 2.3299\n",
      "epoch: 1/4, step: 2001/12500, loss: 2.3243\n",
      "epoch: 1/4, step: 3001/12500, loss: 2.2863\n",
      "epoch: 1/4, step: 4001/12500, loss: 2.3008\n",
      "epoch: 1/4, step: 5001/12500, loss: 2.2961\n",
      "epoch: 1/4, step: 6001/12500, loss: 2.2301\n",
      "epoch: 1/4, step: 7001/12500, loss: 2.2989\n",
      "epoch: 1/4, step: 8001/12500, loss: 2.2517\n",
      "epoch: 1/4, step: 9001/12500, loss: 2.4101\n",
      "epoch: 1/4, step: 10001/12500, loss: 2.2153\n",
      "epoch: 1/4, step: 11001/12500, loss: 2.0035\n",
      "epoch: 1/4, step: 12001/12500, loss: 2.1404\n",
      "epoch: 2/4, step: 1/12500, loss: 2.1014\n",
      "epoch: 2/4, step: 1001/12500, loss: 1.9343\n",
      "epoch: 2/4, step: 2001/12500, loss: 2.4144\n",
      "epoch: 2/4, step: 3001/12500, loss: 1.6064\n",
      "epoch: 2/4, step: 4001/12500, loss: 1.9774\n",
      "epoch: 2/4, step: 5001/12500, loss: 2.2034\n",
      "epoch: 2/4, step: 6001/12500, loss: 1.5898\n",
      "epoch: 2/4, step: 7001/12500, loss: 1.5233\n",
      "epoch: 2/4, step: 8001/12500, loss: 1.3613\n",
      "epoch: 2/4, step: 9001/12500, loss: 1.7948\n",
      "epoch: 2/4, step: 10001/12500, loss: 1.3924\n",
      "epoch: 2/4, step: 11001/12500, loss: 1.3730\n",
      "epoch: 2/4, step: 12001/12500, loss: 2.0218\n",
      "epoch: 3/4, step: 1/12500, loss: 1.7389\n",
      "epoch: 3/4, step: 1001/12500, loss: 1.1101\n",
      "epoch: 3/4, step: 2001/12500, loss: 1.6224\n",
      "epoch: 3/4, step: 3001/12500, loss: 1.6977\n",
      "epoch: 3/4, step: 4001/12500, loss: 1.7184\n",
      "epoch: 3/4, step: 5001/12500, loss: 2.1840\n",
      "epoch: 3/4, step: 6001/12500, loss: 1.3518\n",
      "epoch: 3/4, step: 7001/12500, loss: 1.9994\n",
      "epoch: 3/4, step: 8001/12500, loss: 1.3937\n",
      "epoch: 3/4, step: 9001/12500, loss: 1.5282\n",
      "epoch: 3/4, step: 10001/12500, loss: 1.2015\n",
      "epoch: 3/4, step: 11001/12500, loss: 1.8014\n",
      "epoch: 3/4, step: 12001/12500, loss: 2.0240\n",
      "epoch: 4/4, step: 1/12500, loss: 1.6948\n",
      "epoch: 4/4, step: 1001/12500, loss: 1.1280\n",
      "epoch: 4/4, step: 2001/12500, loss: 1.8025\n",
      "epoch: 4/4, step: 3001/12500, loss: 0.8680\n",
      "epoch: 4/4, step: 4001/12500, loss: 1.3072\n",
      "epoch: 4/4, step: 5001/12500, loss: 0.9282\n",
      "epoch: 4/4, step: 6001/12500, loss: 1.3949\n",
      "epoch: 4/4, step: 7001/12500, loss: 1.0765\n",
      "epoch: 4/4, step: 8001/12500, loss: 1.0271\n",
      "epoch: 4/4, step: 9001/12500, loss: 1.8303\n",
      "epoch: 4/4, step: 10001/12500, loss: 1.7954\n",
      "epoch: 4/4, step: 11001/12500, loss: 1.6970\n",
      "epoch: 4/4, step: 12001/12500, loss: 1.0649\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred_labels = model(images)\n",
    "        loss = criterion(pred_labels, labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"epoch: {epoch+1}/{num_epochs}, step: {i+1}/{n_total_steps}, loss: {loss.item():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of network: 0.01 %\n",
      "accuracy of plane: 42.8 %\n",
      "accuracy of car: 67.2 %\n",
      "accuracy of bird: 20.7 %\n",
      "accuracy of cat: 18.1 %\n",
      "accuracy of deer: 51.5 %\n",
      "accuracy of dog: 36.8 %\n",
      "accuracy of frog: 65.6 %\n",
      "accuracy of horse: 44.2 %\n",
      "accuracy of ship: 65.9 %\n",
      "accuracy of truck: 52.1 %\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "with torch.no_grad():\n",
    "\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        # reshape -> same as above\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred_labels = model(images)\n",
    "\n",
    "        # pred_labels would be probabilities\n",
    "        # get classes out of those prob.\n",
    "        _, pred_labels = torch.max(pred_labels, dim=1)\n",
    "\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct = (pred_labels == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred_label = pred_labels[i]\n",
    "            if pred_label == label:\n",
    "                n_class_correct[label]+=1\n",
    "            n_class_samples[label]+=1\n",
    "\n",
    "    accuracy = 100 * n_correct / n_samples\n",
    "    print(f\"accuracy of network: {accuracy} %\")\n",
    "\n",
    "    for i in range(10):\n",
    "        accuracy = 100 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f\"accuracy of {classes[i]}: {accuracy} %\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}