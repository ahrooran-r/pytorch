{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA GeForce GTX 1650 with Max-Q Design'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "gpu = torch.cuda.current_device()\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do a gradient descent completely manually with numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# linear regression has formula f(x) = w * x\n",
    "# so pytorch has to actually predict w\n",
    "\n",
    "# let us say f(x) = 2 * x\n",
    "# (pytorch doesn't know this. it has to predict that w=2)\n",
    "X = np.array([1,2,3,4,5,6,7,8], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8,10,12,14,16], dtype=np.float32)\n",
    "\n",
    "# initially set w to 0\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss: MSE = 1/N * (wx - y)**2\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# dloss/dw = 1/N * 2 * (wx-y) * x\n",
    "# dloss/dw = 1/N * 2 * x . (wx-y) <- dot product\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2 * x, y_predicted - y).mean()\n",
    "\n",
    "# training\n",
    "learning_rate = 0.001\n",
    "n_iters = 25\n",
    "\n",
    "print(f\"Prediction before training for f(5) == {forward(5)}\")\n",
    "for epoch in range(n_iters):\n",
    "\n",
    "    # prediction -> forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # now update weights based on gradients\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    # print info\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f\"Prediction after training for f(5) == {forward(5)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training for f(5) == 0.0\n",
      "epoch 1: w = 0.816, loss = 102.00000000\n",
      "epoch 3: w = 1.585, loss = 12.52815056\n",
      "epoch 5: w = 1.855, loss = 1.53877044\n",
      "epoch 7: w = 1.949, loss = 0.18899959\n",
      "epoch 9: w = 1.982, loss = 0.02321389\n",
      "epoch 11: w = 1.994, loss = 0.00285125\n",
      "epoch 13: w = 1.998, loss = 0.00035020\n",
      "epoch 15: w = 1.999, loss = 0.00004301\n",
      "epoch 17: w = 2.000, loss = 0.00000528\n",
      "epoch 19: w = 2.000, loss = 0.00000065\n",
      "epoch 21: w = 2.000, loss = 0.00000008\n",
      "epoch 23: w = 2.000, loss = 0.00000001\n",
      "epoch 25: w = 2.000, loss = 0.00000000\n",
      "Prediction after training for f(5) == 9.999979684352876\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets replace gradient calculation with builtin methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training for f(5) == 0.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float32' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\MSIUSE~1\\AppData\\Local\\Temp/ipykernel_14396/602477068.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[1;31m# gradients == backward pass == dl/dw\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m     \u001B[0ml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m     \u001B[1;31m# these backward values will be accumulated in w.grad (dw = w.grad)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.float32' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3,4,5,6,7,8], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8,10,12,14,16], dtype=torch.float32)\n",
    "\n",
    "# instead of writing gradient function myself, use existing method\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss: MSE = 1/N * (wx - y)**2\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "# training\n",
    "learning_rate = 0.001\n",
    "n_iters = 100\n",
    "\n",
    "print(f\"Prediction before training for f(5) == {forward(5)}\")\n",
    "for epoch in range(n_iters):\n",
    "\n",
    "    # prediction -> forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients == backward pass == dl/dw\n",
    "    l.backward()\n",
    "    # these backward values will be accumulated in w.grad (dw = w.grad)\n",
    "\n",
    "    # now update weights based on gradients\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # now reset gradients\n",
    "    w.grad.zero_()\n",
    "\n",
    "    # print info\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f\"Prediction after training for f(5) == {forward(5)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The grad of torch is not as accurate as numerical gradient I created manually\n",
    "So need more steps to get correct answer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets replace other manual functions with pytorch builtins\n",
    "\n",
    "Normal pytorch design paradigm:\n",
    "\n",
    "1. Design model (input, output, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    1. forward pass: compute prediction\n",
    "    2. backward pass: gradients\n",
    "    3. update weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training for f(5) == 4.6803364753723145\n",
      "epoch 1: w = 1.003, loss = 9.04720402\n",
      "epoch 11: w = 1.153, loss = 6.46027613\n",
      "epoch 21: w = 1.280, loss = 4.61313438\n",
      "epoch 31: w = 1.388, loss = 3.29422188\n",
      "epoch 41: w = 1.478, loss = 2.35247684\n",
      "epoch 51: w = 1.555, loss = 1.68004370\n",
      "epoch 61: w = 1.620, loss = 1.19990563\n",
      "epoch 71: w = 1.675, loss = 0.85707104\n",
      "epoch 81: w = 1.721, loss = 0.61227602\n",
      "epoch 91: w = 1.760, loss = 0.43748266\n",
      "Prediction after training for f(5) == 8.967496871948242\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# model prediction\n",
    "# use pytorch models -> no need for weights anymore\n",
    "model = nn.Linear(in_features=input_size, out_features=output_size)\n",
    "\n",
    "# replace loss with built in torch methods\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# training\n",
    "learning_rate = 0.001\n",
    "n_iters = 100\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Prediction before training for f(5) == {model(X_test).item()}\")\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "\n",
    "    # prediction -> forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients == backward pass == dl/dw\n",
    "    l.backward()\n",
    "    # these backward values will be accumulated in w.grad (dw = w.grad)\n",
    "\n",
    "    # now update weights based on gradients\n",
    "    # automatic\n",
    "    optimizer.step()\n",
    "\n",
    "    # now reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # print info\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f\"Prediction after training for f(5) == {model(X_test).item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# suppose we want a custom model\n",
    "class MyLinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MyLinearRegression, self).__init__()\n",
    "\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.lin(X)\n",
    "\n",
    "model = MyLinearRegression(4, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}